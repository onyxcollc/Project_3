{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Kris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Twitter API imports\n",
    "import tweepy as tw\n",
    "\n",
    "# NLP & Sentiment imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download/Update the VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Authentication Verified\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Authenticates the Alpaca API and Twitter\n",
    "    Returns a pass/fail statement\n",
    "\"\"\"\n",
    "############################################################  \n",
    "    \n",
    "# Setting twitter access and api keys\n",
    "bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\")\n",
    "consumer_key= os.getenv(\"TWITTER_API_KEY\")\n",
    "consumer_secret= os.getenv(\"TWITTER_SECRET_KEY\")\n",
    "access_token= os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "access_token_secret= os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "\n",
    "# authentication for twitter\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "twitter_api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# test authentication\n",
    "try:\n",
    "    twitter_api.verify_credentials()\n",
    "    auth = \"Twitter Authentication Verified\"\n",
    "except:\n",
    "    auth = \"Error During Twitter Authentication\"\n",
    "    \n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Function that pulls stock data for every ticker symbol on specified exchange\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "#def sp500_ticker_list():\n",
    "# scrapes the wikipedia page relating to the S&P 500 and returns a list of DataFrame objects\n",
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "# Since we are only interested in the current list of stocks in the S&P 500, we only need the DataFrame object at index 0\n",
    "sp500_list_df = table[0]\n",
    "sp500_tickers = pd.DataFrame(sp500_list_df[[\"Symbol\", \"Security\"]])\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Twitter: Scrape Tweets and Analyze Sentiment\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def twitter_sentiment(search_word, date_since, items):\n",
    "       \n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # initializing the tweets dataframe\n",
    "    df = []\n",
    "    \n",
    "    # adding retweet filter to search words\n",
    "    search_word = search_word + \" -filter:retweets\"\n",
    "    \n",
    "    # Fetch top tweets/hastags for given ticker\n",
    "    tweets = tw.Cursor(twitter_api.search,\n",
    "              q=search_word,\n",
    "              lang=\"en\",\n",
    "              since=date_since\n",
    "                    ).items(items)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "    \n",
    "        #storing tweet text\n",
    "        tweet_fetched = tweet.text \n",
    "\n",
    "        # Get date of tweet\n",
    "        tweet_date = pd.Timestamp(tweet.created_at, tz=\"America/New_York\").isoformat()\n",
    "        \n",
    "        try:\n",
    "            sentiment = analyzer.polarity_scores(tweet_fetched)\n",
    "            compound = sentiment[\"compound\"]\n",
    "        \n",
    "            df.append({\n",
    "                \"date\": tweet_date,\n",
    "                \"tweet\": tweet_fetched,\n",
    "                \"compound\": compound,\n",
    "            })\n",
    "        \n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    get_twitter_sentiment(search_words):\n",
    "    Takes 2 argument, word(s) to search tweets for, and number of items (tweets) to return.\n",
    "    \n",
    "    Scrapes Twitter for given search words in tweet\n",
    "    Calculates compound sentiment with VADER sentiment analyzer on each tweet\n",
    "    Calculates average compound sentiment score each 1 hour\n",
    "    Normalizes average hourly VADER compound score\n",
    "    Returns Average Hourly Sentiment Dataframe with Columns: \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def get_avg_twitter_sentiment(ticker, search_word):\n",
    "\n",
    "    # tweepy variables\n",
    "    search_word = search_word\n",
    "    date_since = \"2021-01-01\"\n",
    "    items = 500\n",
    "    twitter_search_phrase = search_word + \" OR \" + ticker\n",
    "\n",
    "    # call the twitter sentiment function and return a dataframe\n",
    "    tweets_df = twitter_sentiment(twitter_search_phrase, date_since, items)\n",
    "\n",
    "    # Changes the date column to proper datetime format\n",
    "    tweets_df[\"date\"] = pd.to_datetime(tweets_df['date'])\n",
    "    \n",
    "    # Grouping the tweets by Hour and taking their average Hourly sentiment\n",
    "    avg_sentiment = tweets_df.groupby(pd.Grouper(key='date', freq='H')).mean().dropna()\n",
    "\n",
    "    \n",
    "    return avg_sentiment\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Runs a VADER twitter sentiment for a given ticker symbol.\n",
    "    Returns a dataframe with the ticker symbol and average compound VADER score for a time period of 15 minutes.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def compound_twitter_sentiment(ticker, search_word):\n",
    "    \n",
    "    latest_twitter_sentiment = get_avg_twitter_sentiment(ticker, search_word)\n",
    "    twitter_sentiment_score = latest_twitter_sentiment.iloc[-1]\n",
    "    \n",
    "    return twitter_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compound    0.13209\n",
       "Name: 2021-02-28 10:00:00-05:00, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing\n",
    "\n",
    "\n",
    "#ticker = \"MSFT\"\n",
    "#search_word = \"Microsoft\"\n",
    "\n",
    "#score = compound_twitter_sentiment(ticker, search_word)\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
