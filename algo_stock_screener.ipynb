{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Authentication Verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Kris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Importing initial Notebooks and Libraries\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "# Jupyter notebook imports\n",
    "%run twitter_sentiment.ipynb\n",
    "\n",
    "# Library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import alpaca_trade_api as tradeapi\n",
    "from talib import RSI, OBV, ATR, STDDEV\n",
    "\n",
    "# Load .env enviroment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    API Authentications for Alpaca\n",
    "\"\"\"\n",
    "############################################################\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version = \"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators(symbol):   \n",
    "\n",
    "    indicator_scores = []\n",
    "    column_names = ['symbol']\n",
    "    indicator_scores = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    # Set the symbol row and column\n",
    "    indicator_scores.loc[0, 'symbol'] = symbol\n",
    "    \n",
    "    # Set timeframe to 15 Minutes\n",
    "    timeframe = \"15Min\"\n",
    "    \n",
    "    # Set end date to now for latest data\n",
    "    end_date = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "    \n",
    "    # set limit to past 50 15-minute intervals (equal to one trading day of 6.5 hours)\n",
    "    limit = 50\n",
    "        \n",
    "    #fetch ticker data \n",
    "    ticker_data = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit,\n",
    "        end=end_date\n",
    "        )[ticker]._raw\n",
    "    ticker_data_df = pd.DataFrame(data=ticker_data)\n",
    "    \n",
    "    ############################################################\n",
    "    \"\"\" \n",
    "        Volatility - Average True Range\n",
    "        Finds volatile stocks using ATR.\n",
    "        Looks for stocks that typically move more than 5% per day, based on a 50 time period average, \n",
    "        \n",
    "        Relative strength suggests continued outperformance while relative weakness suggests continued underperformance.\n",
    "        High RSI (usually above 70) may indicate a stock is overbought, therefore it is a sell signal. \n",
    "        Low RSI (usually below 30) indicates stock is oversold, which means a buy signal. \n",
    "        \n",
    "        On-balance volume (OBV) is a technical trading momentum indicator that \n",
    "        uses volume flow to predict changes in stock price.\n",
    "        \n",
    "        When both price and OBV are making higher peaks and higher troughs, \n",
    "        the upward trend is likely to continue.\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    \n",
    "    #fetch Average True Range(ATR) volatility indicator\n",
    "    #average_true_range = pd.DataFrame(ATR(ticker_data_df['h'], ticker_data_df['l'], ticker_data_df['c'], timeperiod=14))\n",
    "\n",
    "    #fetch Standard Deviation volatility indicator\n",
    "    #std_dev = pd.DataFrame(STDDEV(ticker_data_df['c'], timeperiod=5, nbdev=1))\n",
    "    std_dev = pd.DataFrame(ticker_data_df['c']).std()\n",
    "    std_dev = std_dev.mean()\n",
    "                           \n",
    "    #Fetch RSI score\n",
    "    rsi_score = pd.DataFrame(RSI(ticker_data_df['c'], timeperiod=14))\n",
    "    rsi_score = rsi_score.mean()\n",
    "     \n",
    "    #Fetch On Balance Volume(OBV)\n",
    "    obv_score = pd.DataFrame(OBV(ticker_data_df['c'], ticker_data_df['v']))\n",
    "    \n",
    "    if ((rsi_score <= 45) | (std_dev > 0.008554)).all(): \n",
    "    \n",
    "        # if all conditions met, appends scores to dataframe\n",
    "        indicator_scores.loc[0, 'close'] = ticker_data_df.loc[(len(ticker_data_df)-1), 'c']\n",
    "        indicator_scores.loc[0, 'std_dev'] = std_dev #.loc[0]                     \n",
    "        #indicator_scores.loc[0, 'avg_true_range'] = average_true_range.loc[(len(average_true_range)-1), 0]\n",
    "        indicator_scores.loc[0, 'relative_strength_index'] = rsi_score.loc[0] #.loc[(len(rsi_score)-1), 0]\n",
    "        indicator_scores.loc[0, 'on_balance_volume'] = obv_score.loc[(len(obv_score)-1), 0]\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    indicator_scores.dropna(inplace=True)\n",
    "    \n",
    "    return indicator_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'compound'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'compound'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-e65f438bde2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m#fetches the Compound Sentiment score for the last 15 minutes of Tweets on Twitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtwitter_comp_sentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecurity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mindicators_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'twitter_sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_comp_sentiment_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# need to update code, if there aren't enough tweets to create a compound score, set to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-272-e65f438bde2e>\u001b[0m in \u001b[0;36mfetch_twitter_sentiment\u001b[0;34m(ticker, search_word)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompound_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-976117220a74>\u001b[0m in \u001b[0;36mcompound_twitter_sentiment\u001b[0;34m(ticker, search_word)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mtwitter_sentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatest_twitter_sentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compound\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#.loc[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# need to update code, if there aren't enough tweets to create a compound score, set to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'compound'"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Function that fetches the Twitter Sentiment score from twitter_sentiment.ipynb\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def fetch_twitter_sentiment(ticker, search_word):\n",
    "    score = compound_twitter_sentiment(ticker, search_word)\n",
    "    return score\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Function scrapes Wikipedia and pulls stock data for every ticker symbol on the S&P500\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "# scrapes the wikipedia page relating to the S&P 500 and returns a list of DataFrame objects\n",
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "# Since we are only interested in the current list of stocks in the S&P 500, we only need the DataFrame object at index 0\n",
    "sp500_list_df = table[0]\n",
    "sp500_tickers = sp500_list_df[[\"Symbol\", \"Security\"]]\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Stock Screener for Day Trading\n",
    "    Start Main Function\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "# Initialize the dataframe and set the columns\n",
    "indicators_df = []\n",
    "column_names = ['symbol', 'close', 'std_dev', 'relative_strength_index', 'on_balance_volume', 'twitter_sentiment']\n",
    "indicators_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# for loop iterates through all tickers in S&P 500\n",
    "for tickers in range(len(sp500_tickers[\"Symbol\"])):\n",
    "    \n",
    "    # set the current ticker symbol and company security name\n",
    "    company = sp500_tickers.loc[[tickers][0]]\n",
    "    ticker = company.Symbol\n",
    "    security = company.Security\n",
    "    \n",
    "    # Set timeframe to 1 Day to capture daily Volume total\n",
    "    timeframe = \"1D\"\n",
    "    \n",
    "    # Set the limit of bars to just the previous time period\n",
    "    limit = 1\n",
    "\n",
    "    # Set end date to now for latest data (if run before market open to fetch yesterdays close)\n",
    "    end_date = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "    \n",
    "    # Initial pass of ticker symbols\n",
    "    # fetch last daily barset object for ticker and put raw data into dataframe\n",
    "    bar_set = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit,\n",
    "        end=end_date\n",
    "        )[ticker]._raw\n",
    "    initial_check = pd.DataFrame(data=bar_set)\n",
    "    \n",
    "    ############################################################\n",
    "    \"\"\"\n",
    "    # Since Day traders generally look for stocks that have at least 1 million shares traded daily, \n",
    "    # this checks to see if the daily volume for current ticker >= 1Million.\n",
    "    # This also checks the last closing price of a ticker to see if it is below $1 or above $100.\n",
    "    # If ticker volume < 1M or it's last closing price was above $100, \n",
    "    # we stop this iteration and continue to the next iteration of the loop.\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "           \n",
    "    if initial_check.loc[0, 'v'] <= 1000000 or initial_check.loc[0, 'c'] > 100 or initial_check.loc[0, 'c'] < 1:\n",
    "        continue\n",
    "    else:\n",
    "        # If the ticker passes initial checks, it continues to calculating the additional indicators\n",
    "        \n",
    "        indicators_df = indicators_df.append(get_indicators(ticker), ignore_index=True)\n",
    "        \n",
    "        \n",
    "        ############################################################\n",
    "        \"\"\" \n",
    "            Twitter and Reddit Compound Sentiment\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        \n",
    "        \n",
    "        #fetches the Compound Sentiment score for the last 15 minutes of Tweets on Twitter\n",
    "        twitter_comp_sentiment_score = fetch_twitter_sentiment(ticker, security)\n",
    "        indicators_df.loc[tickers, 'twitter_sentiment'] = twitter_comp_sentiment_score\n",
    "        # need to update code, if there aren't enough tweets to create a compound score, set to 0\n",
    "            \n",
    "        #fetch reddit sentiment\n",
    "        #reddit_sentiment = fetch_reddit_sentiment(ticker)\n",
    "        #sp500_ticker_data.loc[x, 'reddit_sentiment'] = reddit_sentiment\n",
    "\n",
    "\n",
    "indicators_df.set_index('symbol', inplace=True)\n",
    "indicators_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\" \n",
    "\n",
    "    Adds up scores of each indicator and returns a sorted list with Top 5 stock tickers\n",
    "    \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "\n",
    "# adds up sentiment indicator column scores and calculates the total_indi_score\n",
    "#for symbol in range(len(indicators_df)):\n",
    "    #indicators_df[symbol, 'total_sentiment_score'] = float(indicators_df[rows, 'twitter_sentiment'] + indicators_df[rows, 'reddit_sentiment'])\n",
    "    \n",
    "# sorts indidcator_df by total_indie_score\n",
    "#indicators_df.sort_by(by='total_trading_score', ascending=True)\n",
    "\n",
    "# saves top 5 tickers into top5_stocks_df[\"tickers\"]\n",
    "#top5_stocks_df = indicators_df[:4]\n",
    "\n",
    "# return top 5 stock recommendations\n",
    "#top5_stocks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ############################################################\n",
    "        \"\"\"         \n",
    "        \n",
    "        # Set timeframe to 15 Minutes\n",
    "        timeframe = \"15Min\"\n",
    "    \n",
    "        #fetch ticker data \n",
    "        ticker_data = api.get_barset(\n",
    "            ticker,\n",
    "            timeframe,\n",
    "            limit,\n",
    "            end=end_date\n",
    "            )\n",
    "    \n",
    "        # ticker_data inherits dict and bars inherits list. You can iterate over them accordingly: \n",
    "        for symbols in ticker_data:\n",
    "            bars = ticker_data[symbols]\n",
    "            for bar in bars:\n",
    "                sp500_ticker_data.loc[symbols, 'time'] = bar.t\n",
    "                sp500_ticker_data.loc[symbols, 'open'] = bar.o\n",
    "                sp500_ticker_data.loc[symbols, 'low'] = bar.l\n",
    "                sp500_ticker_data.loc[symbols, 'high'] = bar.h\n",
    "                sp500_ticker_data.loc[symbols, 'close'] = bar.c\n",
    "                sp500_ticker_data.loc[symbols, 'volume'] = bar.v\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
