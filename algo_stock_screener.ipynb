{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Authentication Verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Kris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'talib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-7c5c837272ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malpaca_trade_api\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtradeapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtalib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRSI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOBV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load .env enviroment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talib'"
     ]
    }
   ],
   "source": [
    "# Importing notebook dependencies\n",
    "%run twitter_sentiment.ipynb\n",
    "\n",
    "#imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import alpaca_trade_api as tradeapi\n",
    "from talib import RSI, OBV\n",
    "\n",
    "# Load .env enviroment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api authentications\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version = \"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\"\n",
    "    Function scrapes Wikipedia and pulls stock data for every ticker symbol on the S&P500\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "# scrapes the wikipedia page relating to the S&P 500 and returns a list of DataFrame objects\n",
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "# Since we are only interested in the current list of stocks in the S&P 500, we only need the DataFrame object at index 0\n",
    "sp500_list_df = table[0]\n",
    "sp500_tickers = sp500_list_df[[\"Symbol\", \"Security\"]]\n",
    "\n",
    "############################################################\n",
    "\"\"\"\n",
    "    Function that fetches the Twitter Sentiment score\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def fetch_twitter_sentiment(ticker, search_word):\n",
    "    score = compound_twitter_sentiment(ticker, search_word)\n",
    "    return score\n",
    "\n",
    "############################################################\n",
    "\"\"\" \n",
    "    Relative strength suggests continued outperformance while relative weakness suggests continued underperformance.\n",
    "    High RSI (usually above 70) may indicate a stock is overbought, therefore it is a sell signal. \n",
    "    Low RSI (usually below 30) indicates stock is oversold, which means a buy signal. \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def fetch_rsi(close)\n",
    "    timeperiod=1\n",
    "    rsi = RSI(close, timeperiod)\n",
    "    return rsi\n",
    "\n",
    "############################################################\n",
    "\"\"\" \n",
    "    On-balance volume (OBV) is a technical trading momentum indicator that \n",
    "    uses volume flow to predict changes in stock price.\n",
    "    \n",
    "    When both price and OBV are making higher peaks and higher troughs, \n",
    "    the upward trend is likely to continue.\n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "def fetch_obv(close, volume)\n",
    "    obv = OBV(close, volume)\n",
    "    return obv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 657\n",
      "Rate limit reached. Sleeping for: 852\n",
      "Rate limit reached. Sleeping for: 850\n",
      "Rate limit reached. Sleeping for: 849\n",
      "Rate limit reached. Sleeping for: 851\n",
      "Rate limit reached. Sleeping for: 852\n",
      "Rate limit reached. Sleeping for: 853\n",
      "Rate limit reached. Sleeping for: 852\n",
      "Rate limit reached. Sleeping for: 850\n",
      "Rate limit reached. Sleeping for: 850\n",
      "Rate limit reached. Sleeping for: 850\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-a22b1f8e7b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#fetches the Compound Sentiment score for the last 15 minutes of Tweets on Twitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtwitter_comp_sentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecurity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0msp500_ticker_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'twitter_sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_comp_sentiment_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-79ee18948339>\u001b[0m in \u001b[0;36mfetch_twitter_sentiment\u001b[0;34m(ticker, search_word)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompound_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-c404179bef95>\u001b[0m in \u001b[0;36mcompound_twitter_sentiment\u001b[0;34m(ticker, search_word)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompound_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mlatest_twitter_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_avg_twitter_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mtwitter_sentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatest_twitter_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-c404179bef95>\u001b[0m in \u001b[0;36mget_avg_twitter_sentiment\u001b[0;34m(ticker, search_word)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Changes the date column to proper datetime format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Grouping the tweets by Hour and taking their average Hourly sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlpenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# Initialize the dataframe and set the columns\n",
    "sp500_ticker_data = []\n",
    "column_names = ['time', 'open', 'low', 'close', 'volume', 'rsi', 'obv', 'volatility', 'twitter_sentiment', 'reddit_sentiment']\n",
    "sp500_ticker_data = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# initialize and set counter x to 0\n",
    "x=0\n",
    "\n",
    "for tickers in range(len(sp500_tickers[\"Symbol\"])):\n",
    "    \n",
    "    # set the current ticker symbol then increases x by 1 \n",
    "    company = sp500_tickers.loc[[x][0]]\n",
    "    ticker = company.Symbol\n",
    "    security = company.Security\n",
    "    \n",
    "    # Set timeframe to 15 Minutes\n",
    "    timeframe = \"D\"\n",
    "    \n",
    "    # Set the limit of bars to just the previous 15-minute time period\n",
    "    limit = 1\n",
    "\n",
    "    # Set date to today and end datetimes of 1 day before (if run before market open to fetch yesterdays close)\n",
    "    end_date = pd.Timestamp.now(tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    #fetch ticker data and into temp. dataframe\n",
    "    volume_check = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit,\n",
    "        end=end_date\n",
    "        )\n",
    "    \n",
    "    # Since Day traders generally look for stocks that have at least 1 million shares traded daily, \n",
    "    # this checks to see if the daily volume for current ticker >= 1Million.\n",
    "    # If it is < 1M, we stop this iteration and continue to the next iteration of the loop.\n",
    "    if volume_check.bar.v <= 1000000: #1Million\n",
    "        continue\n",
    "    else:\n",
    "        # Set timeframe to 15 Minutes\n",
    "        timeframe = \"15Min\"\n",
    "    \n",
    "        #fetch ticker data \n",
    "        ticker_data = api.get_barset(\n",
    "            ticker,\n",
    "            timeframe,\n",
    "            limit,\n",
    "            end=end_date\n",
    "            )\n",
    "    \n",
    "        #ticker_data inherits dict and bars inherits list. You can iterate over them accordingly\n",
    "        for symbols in ticker_data:\n",
    "            bars = ticker_data[symbols]\n",
    "            for bar in bars:\n",
    "                sp500_ticker_data.loc[symbols, 'time'] = bar.t\n",
    "                sp500_ticker_data.loc[symbols, 'open'] = bar.o\n",
    "                sp500_ticker_data.loc[symbols, 'low'] = bar.l\n",
    "                sp500_ticker_data.loc[symbols, 'close'] = bar.c\n",
    "                sp500_ticker_data.loc[symbols, 'volume'] = bar.v\n",
    "        \n",
    "        #Fetch RSI score\n",
    "        rsi_score = fetch_rsi(sp500_ticker_data.loc[x, 'close'])\n",
    "        sp500_ticker_data.loc[x, 'rsi'] = rsi_score\n",
    "        \n",
    "        #Fetch On Balance Volume(OBV)\n",
    "        obv_score = fetch_obv(sp500_ticker_data.loc[x, 'close'], sp500_ticker_data.loc[x, 'volume'])\n",
    "        sp500_ticker_data.loc[x, 'obv'] = obv_score\n",
    "        \n",
    "        #fetch volatility\n",
    "        \n",
    "        #fetches the Compound Sentiment score for the last 15 minutes of Tweets on Twitter\n",
    "        twitter_comp_sentiment_score = fetch_twitter_sentiment(ticker, security)\n",
    "        sp500_ticker_data.loc[x, 'twitter_sentiment'] = twitter_comp_sentiment_score\n",
    "        \n",
    "        #fetch reddit sentiment\n",
    "        #reddit_sentiment = fetch_reddit_sentiment(ticker)\n",
    "        #sp500_ticker_data.loc[x, 'rsi'] = reddit_sentiment\n",
    "        \n",
    "        \n",
    "        #increment x by 1 for next loop\n",
    "        x+=1\n",
    "    \n",
    "sp500_ticker_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\" \n",
    "    Adds up scores of each indicator and returns a sorted list with Top 5 stock tickers\n",
    "    \n",
    "\"\"\"\n",
    "############################################################\n",
    "\n",
    "# indicators_df = pd.DataFrame(sp500_ticker_data)\n",
    "\n",
    "# adds up all indicator column scores and calculates the total_indi_score\n",
    "#for rows in range(len(indicators_df))\n",
    "    #indicators_df[rows, 'total_trading_score'] = float(indicators_df[rows, 'rsi'] + indicators_df[rows, 'obv'] + indicators_df[rows, 'volatility'] + indicators_df[rows, 'twitter_sentiment'] + indicators_df[rows, 'reddit_sentiment'])\n",
    "    \n",
    "# sorts indidcator_df by total_indie_score\n",
    "# indicators_df.sort_by(by='total_trading_score', ascending=True)\n",
    "\n",
    "# saves top 5 tickers into top5_stocks_df[\"tickers\"]\n",
    "# top5_stocks_df = indicators_df[:4]\n",
    "\n",
    "#return top 5 stock recommendations\n",
    "#return top5_stocks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
